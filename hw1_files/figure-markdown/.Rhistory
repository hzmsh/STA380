#summary(airb)
nrow(airb) #9506
library (MASS)
Boston
?Boston
dim(Boston)
summary(Boston)
#(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.
attach(Boston)
pairs(Boston)
Boston_corr = cor(Boston)
Boston_corr_crim = Boston_corr[-1,1]
print(Boston_corr_crim[order(abs(Boston_corr_crim), decreasing = T)])
par(mfrow=c(2,3))
plot(rad, crim)
plot(tax,crim)
plot(lstat,crim)
plot(rox,crim)
plot(indus,crim)
# Crim:
hist(crim)
summary(crim)
length(crim[crim>40])
rm(list=ls())
library(MASS)
attach(Boston)
par(mfrow=c(7,2))
boston.zn <- lm(crim ~ zn)
summary(fit.zn)
plot(zn, crim)
abline(boston.zn, col="green")
boston.zn <- lm(crim ~ zn)
summary(fit.zn)
summary(boston.zn)
plot(zn, crim)
abline(boston.zn, col="green")
dev.off()
plot(zn, crim)
abline(boston.zn, col="green")
boston.indus<-lm(crim~indus,data=Boston)
boston.indux
boston.indus
summary(boston.indus)
plot(indus, crim)
abline(boston.indus, col="green")
rm(list=ls())
library(ISLR)
college <- College
attach(college)
head(college,10)
# Dividing data into 70% training and 30% test set.
# Seeting seed
set.seed(123)
#Sample Indexes
indexes=sample(1:nrow(college),size=0.3*nrow(college))
# Split data, 70% training & 30% test
#Setting up training set
train=college[-indexes,]
#Setting up test set
test=college[indexes,]
#Removing unused data
rm(college)
# (b) Fit a linear model using least squares on the training set, and report the test error obtained.
#Including library necessary for predict function
library(caret)
# (b) Fit a linear model using least squares on the training set, and report the test error obtained.
#Including library necessary for predict function
library(lattice)
library(ggplot2)
(caret)
library(caret)
#Creating training model using logistic regression
model=lm(Apps~.,data=train)
#Printing out the logistic model
summary(model)
model
test
# Fitting model to the test set and checking accuracy. Below is Mean Squared Error.
#Fitting trainning model on test set
pred=predict(model,newdata=test)
#Calculating Accuracy
MSE=mean((test$Apps-pred)^2)
#Printing MSE
print(MSE)
# (c) Fit a ridge regression model on the training set,
# with Î» chosen by cross-validation. Report the test error obtained.
#Including library necessary for ridge regression
library(glmnet)
library(Matrix)
library(foreach)
library(glmnet)
#For ridge regression, we must pass in an x matrix as well as a y vector for train and test sets
xtrain=model.matrix (Apps~.,train)[,-1]
ytrain=train$Apps
xtest=model.matrix (Apps~.,test)[,-1]
ytest=test$Apps
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =0)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using ridge regression
model =glmnet(xtrain,ytrain,alpha=0,lambda=bestlam)
#Printing out the logistic model
model$beta
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =1)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =glmnet(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
#Fitting trainning model on test set
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
lasscoef=predict(model,type="coefficients",s=bestlam)[1:length(model$beta),]
#Printing non zero coefficients
lasscoef[lasscoef!=0]
library(ncvreg)
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.ncvreg (xtrain,ytrain,alpha =1,penalty=c("SCAD"))
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =ncvreg(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
#Fitting trainning model on test set
pred=predict(model,xtest,s=bestlam)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
par(mar=c(1,1,1,1))
for (i in seq(0, 1, .1))
{
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =i)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =glmnet(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
}
#Fitting trainning model on test set
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
head(college,10)
rm(list=ls())
library(ISLR)
college <- College
attach(college)
head(college,10)
View(college)
#Sample Indexes
indexes=sample(1:nrow(college),size=0.3*nrow(college))
indexes
# Split data, 70% training & 30% test
#Setting up training set
train=college[-indexes,]
#Removing unused data
rm(college)
library(lattice)
library(ggplot2)
library(caret)
#Creating training model using logistic regression
model=lm(Apps~.,data=train)
# Split data, 70% training & 30% test
#Setting up training set
train=college[-indexes,]
#Printing out the logistic model
summary(model)
pred=predict(model,newdata=test)
#Calculating Accuracy
MSE=mean((test$Apps-pred)^2)
#Printing MSE
print(MSE)
# Fitting model to the test set and checking accuracy. Below is Mean Squared Error.
#Fitting trainning model on test set
pred=predict(model,newdata=test)
library(lattice)
library(ggplot2)
library(caret)
#Creating training model using logistic regression
model=lm(Apps~.,data=train)
#Printing out the logistic model
summary(model)
pred=predict(model,newdata=test)
#Calculating Accuracy
MSE=mean((test$Apps-pred)^2)
#Printing MSE
print(MSE)
#Importing data file
library(ISLR)
college <- College
attach(college)
head(college,10)
set.seed(123)
#Sample Indexes
indexes=sample(1:nrow(college),size=0.3*nrow(college))
# Split data, 70% training & 30% test
#Setting up training set
train=college[-indexes,]
#Setting up test set
test=college[indexes,]
#Removing unused data
rm(college)
library(lattice)
library(ggplot2)
library(caret)
model=lm(Apps~.,data=train)
#Printing out the logistic model
summary(model)
# Fitting model to the test set and checking accuracy. Below is Mean Squared Error.
#Fitting trainning model on test set
pred=predict(model,newdata=test)
#Calculating Accuracy
MSE=mean((test$Apps-pred)^2)
#Printing MSE
print(MSE)
library(Matrix)
library(foreach)
library(glmnet)
#For ridge regression, we must pass in an x matrix as well as a y vector for train and test sets
xtrain=model.matrix (Apps~.,train)[,-1]
ytrain=train$Apps
xtest=model.matrix (Apps~.,test)[,-1]
ytest=test$Apps
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =0)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using ridge regression
model =glmnet(xtrain,ytrain,alpha=0,lambda=bestlam)
#Printing out the logistic model
model$beta
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =1)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =glmnet(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
# Fitting model to the test set and checking accuracy. Below is Mean Squared Error.
#Fitting trainning model on test set
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
lasscoef=predict(model,type="coefficients",s=bestlam)[1:length(model$beta),]
#Printing non zero coefficients
lasscoef[lasscoef!=0]
library(ncvreg)
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.ncvreg (xtrain,ytrain,alpha =1,penalty=c("SCAD"))
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =ncvreg(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
# Fitting model to the test set and checking accuracy. Below is Mean Squared Error.
#Fitting trainning model on test set
pred=predict(model,xtest,s=bestlam)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
par(mar=c(1,1,1,1))
for (i in seq(0, 1, .1))
{
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =i)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =glmnet(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
}
#Fitting trainning model on test set
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
# (f) Fit a PLS model on the training set, with M chosen by crossvalidation.
# Report the test error obtained, along with the value of M selected by cross-validation.
par(mar=c(1,1,1,1))
for (i in seq(0, 1, .1))
(list=ls())
rm(list=ls())
# 9  we will predict the number of applications received using the other variables in the College data set.
# (a) Split the data set into a training set and a test set.
#Importing data file
library(ISLR)
college <- College
attach(college)
head(college,10)
# Dividing data into 70% training and 30% test set.
# Seeting seed
set.seed(123)
#Sample Indexes
indexes=sample(1:nrow(college),size=0.3*nrow(college))
# Split data, 70% training & 30% test
#Setting up training set
train=college[-indexes,]
#Setting up test set
test=college[indexes,]
#Removing unused data
rm(college)
# (b) Fit a linear model using least squares on the training set, and report the test error obtained.
#Including library necessary for predict function
library(lattice)
library(ggplot2)
library(caret)
#Creating training model using logistic regression
model=lm(Apps~.,data=train)
#Printing out the logistic model
summary(model)
# Fitting model to the
#Removing unused data
rm(college)
#Fitting trainning model on test set
pred=predict(model,newdata=test)
#Calculating Accuracy
MSE=mean((test$Apps-pred)^2)
#Printing MSE
print(MSE)
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
data = Carseats
train <- sample(1:nrow(Carseats), nrow(Carseats)/2)
car.train <- Carseats[train, ]
car.test <- Carseats[-train, ]
car.tree <- tree(Sales ~ ., data = car.train)
plot(car.tree)
text(car.tree, pretty=0)
summary(car.tree)
car.pred <- predict(car.tree, newdata = car.test)
mean((car.pred - car.test$Sales)^2)
set.seed(1)
car.cv <- cv.tree(car.tree)
par(mfrow = c(1, 2))
plot(car.cv$size, car.cv$dev, type = "b")
plot(car.cv$k, car.cv$dev, type = "b")
par(mfrow = c(1,1))
car.tree <- tree(Sales ~ ., data = car.train)
plot(car.tree)
text(car.tree, pretty=0)
summary(car.tree)
car.pred <- predict(car.tree, newdata = car.test)
mean((car.pred - car.test$Sales)^2)
set.seed(1)
car.cv <- cv.tree(car.tree)
par(mfrow = c(1, 2))
plot(car.cv$size, car.cv$dev, type = "b")
plot(car.cv$k, car.cv$dev, type = "b")
par(mfrow = c(1,1))
# It looks like 7 is the best size.
prune.car <- prune.tree(car.tree, best = 7)
plot(prune.car)
text(prune.car, pretty = 0)
car.cv <- cv.tree(car.tree)
par(mfrow = c(1, 2))
plot(car.cv$size, car.cv$dev, type = "b")
plot(car.cv$k, car.cv$dev, type = "b")
car.tree <- tree(Sales ~ ., data = car.train)
plot(car.tree)
text(car.tree, pretty=0)
summary(car.tree)
car.pred <- predict(car.tree, newdata = car.test)
mean((car.pred - car.test$Sales)^2)
set.seed(1)
car.cv <- cv.tree(car.tree)
par(mfrow = c(1, 2))
plot(car.cv$size, car.cv$dev, type = "b")
rm(list=ls())
dev.off()
# (a) Split the data set into a training set and a test set.
#Importing data file
library(ISLR)
college <- College
attach(college)
head(college,10)
# Dividing data into 70% training and 30% test set.
# Seeting seed
set.seed(123)
indexes=sample(1:nrow(college),size=0.3*nrow(college))
#Setting up training set
train=college[-indexes,]
#Setting up test set
test=college[indexes,]
#Removing unused data
rm(college)
# (b) Fit a linear model using least squares on the training set, and report the test error obtained.
#Including library necessary for predict function
library(lattice)
library(ggplot2)
library(caret)
#Creating training model using logistic regression
model=lm(Apps~.,data=train)
#Printing out the logistic model
summary(model)
# Fitting model to the test set and checking accuracy. Below is Mean Squared Error.
#Fitting trainning model on test set
pred=predict(model,newdata=test)
#Calculating Accuracy
MSE=mean((test$Apps-pred)^2)
#Printing MSE
print(MSE)
library(Matrix)
library(foreach)
library(glmnet)
#For ridge regression, we must pass in an x matrix as well as a y vector for train and test sets
xtrain=model.matrix (Apps~.,train)[,-1]
ytrain=train$Apps
xtest=model.matrix (Apps~.,test)[,-1]
ytest=test$Apps
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =0)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using ridge regression
model =glmnet(xtrain,ytrain,alpha=0,lambda=bestlam)
#Printing out the logistic model
model$beta
#Fitting trainning model on test set
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE - mean squared error
print(MSE)
# (d) Fit a lasso model on the training set, with Î» chosen by crossvalidation.
# Report the test error obtained, along with the number of non-zero coe???cient estimates
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =1)
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =glmnet(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
# Fitting model to the test set and checking accuracy. Below is Mean Squared Error.
#Fitting trainning model on test set
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
#Retrieving the lasso coefficients
lasscoef=predict(model,type="coefficients",s=bestlam)[1:length(model$beta),]
#Printing non zero coefficients
lasscoef[lasscoef!=0]
#Including library necessary for SCAD regression
library(ncvreg)
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.ncvreg (xtrain,ytrain,alpha =1,penalty=c("SCAD"))
plot(cv.out)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =ncvreg(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
#Fitting trainning model on test set
pred=predict(model,xtest,s=bestlam)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
rm(list=ls())
dev.off()
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
getwd()
setwd("C:/Users/mshch96/Desktop/Predictive Models/R Scripts/R_dir")
getwd()
setwd("C:\Users\mshch96\Desktop\hw1_files\figure-markdown")
setwd("C:\Users\mshch96\Desktop\hw1_files\figure-markdown")
setwd("C:/Users/mshch96/Desktop/hw1_files/figure-markdown")
getwd()
